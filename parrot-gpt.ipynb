{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6319e48a",
   "metadata": {},
   "source": [
    "# Parrot-GPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6da380a",
   "metadata": {},
   "source": [
    "Install PyTorch and HuggingFace Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed85d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch torchvision torchaudio transformers -c pytorch -c nvidia -c apple -c huggingface -y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08aba525",
   "metadata": {},
   "source": [
    "Import needed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31bfee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "731c37a4",
   "metadata": {},
   "source": [
    "Now lets get the model.  We can either run the 1.3 billion paramater model or the 2.7 billion parameter model. Lets do the 2.7B model, which is \"EleutherAI/gpt-neo-2.7B\".  The 1.3B model is \"EleutherAI/gpt-neo-1.3B\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6e795fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14d00f01",
   "metadata": {},
   "source": [
    "This model can be ran on a GPU, but does not have to be. The 2.7B model takes slightly less than 13 GB of Vram.  The 1.3B model takes slighly less than 7.5GB of Vram.  The model will be placed on the GPU if there is one and if there is enough Vram."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab5c2bc2",
   "metadata": {},
   "source": [
    "Install pynvml to take a look at how much VRAM we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bde9e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pynvml in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (11.5.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf1002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_vram = 0.0\n",
    "if torch.cuda.is_available():\n",
    "    from pynvml import *\n",
    "    nvmlInit()\n",
    "    h = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(h)\n",
    "    free_vram = info.free/1048576000\n",
    "    print(\"There is a GPU with \" + str(free_vram) + \"GB of free VRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9834d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"EleutherAI/gpt-neo-2.7B\" and free_vram>13.5:\n",
    "    use_cuda = True\n",
    "    model.to(\"cuda:0\")\n",
    "elif model_name == \"EleutherAI/gpt-neo-1.3B\" and free_vram>7.5:\n",
    "    use_cuda = True\n",
    "    model.to(\"cuda:0\")\n",
    "else:\n",
    "    use_cuda = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bebf440c",
   "metadata": {},
   "source": [
    "Load the tokenizer to prepare the input for GPT3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd79a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca5b4cbf",
   "metadata": {},
   "source": [
    "Enter your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e70b4877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a prompt: What is the meaning of life?\n"
     ]
    }
   ],
   "source": [
    "prompt = str(input(\"Please enter a prompt: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3361b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How long should the generated output be? 200\n"
     ]
    }
   ],
   "source": [
    "output_length = int(input(\"How long should the generated output be? \"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3af23dde",
   "metadata": {},
   "source": [
    "Tokenize the input prompt to prepare it for use with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aed05ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "if use_cuda:\n",
    "    input_ids = input_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a16a0ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "gen_tokens = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4298458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the meaning of life?\n",
      "\n",
      "If the meaning of life can be said to be in its own right, then it's difficult to deny its existence. Without question, life itself is the universe and everything in it, with the exception of something like death, which is simply our realization of the unendingness of the universe and its impermanence. For some people, such as the Hindus, the most meaningful thing to them is to serve others in whatever form they can (Hinduism is a very ancient religion that is still growing and evolving). This was how I thought it should be. The more that I experienced myself, as a Buddhist and as a human being, however, the more I realized I didn't really like how things were. I didn't like the way the world was making me. I was tired of the way the world was making me. I wanted to change, especially for the better.\n",
      "\n",
      "This isn't something new to me. I've always\n"
     ]
    }
   ],
   "source": [
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc7f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_vid",
   "language": "python",
   "name": "gpt_vid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
