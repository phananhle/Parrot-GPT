{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084cd65b",
   "metadata": {},
   "source": [
    "First lets install the correct packages for GPT3.  We are already in the conda environment from jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da380a",
   "metadata": {},
   "source": [
    "First lets install pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed85d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install pytorch torchvision torchaudio -c pytorch -c nvidia -c apple -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf283b7",
   "metadata": {},
   "source": [
    "Now lets install HuggingFace.  It makes using popular Tranformers MUCH easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf424c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c huggingface transformers -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aba525",
   "metadata": {},
   "source": [
    "Lets import the needed packages now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31bfee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c37a4",
   "metadata": {},
   "source": [
    "Now lets get the model.  We can either run the 1.3 billion paramater model or the 2.7 billion parameter model. Lets do the 2.7B model, which is \"EleutherAI/gpt-neo-2.7B\".  The 1.3B model is \"EleutherAI/gpt-neo-1.3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e795fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d00f01",
   "metadata": {},
   "source": [
    "This model can be ran on a GPU, but does not have to be. The 2.7B model takes slightly less than 13 GB of Vram.  The 1.3B model takes slighly less than 7.5GB of Vram.  The model will be placed on the GPU if there is one and if there is enough Vram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c2bc2",
   "metadata": {},
   "source": [
    "Lets install pynvml to take a look at how much VRAM we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bde9e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pynvml in /opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages (11.5.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf1002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_vram = 0.0\n",
    "if torch.cuda.is_available():\n",
    "    from pynvml import *\n",
    "    nvmlInit()\n",
    "    h = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(h)\n",
    "    free_vram = info.free/1048576000\n",
    "    print(\"There is a GPU with \" + str(free_vram) + \"GB of free VRAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9834d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"EleutherAI/gpt-neo-2.7B\" and free_vram>13.5:\n",
    "    use_cuda = True\n",
    "    model.to(\"cuda:0\")\n",
    "elif model_name == \"EleutherAI/gpt-neo-1.3B\" and free_vram>7.5:\n",
    "    use_cuda = True\n",
    "    model.to(\"cuda:0\")\n",
    "else:\n",
    "    use_cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf440c",
   "metadata": {},
   "source": [
    "Now we need to load the tokenizer to prepare the input for GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd79a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5b4cbf",
   "metadata": {},
   "source": [
    "We are almost done. At this point we need to decide what prompt we need to decide what prompt we want the model to continue, as well a how long we want the generated output to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e70b4877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a prompt: Is there a God?\n"
     ]
    }
   ],
   "source": [
    "prompt = str(input(\"Please enter a prompt: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3361b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How long should the generated output be? 100\n"
     ]
    }
   ],
   "source": [
    "output_length = int(input(\"How long should the generated output be? \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af23dde",
   "metadata": {},
   "source": [
    "We now need to tokenize the input prompt to prepare it for use with the model.  If we are using a GPU we will put it on the GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aed05ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "if use_cuda:\n",
    "    input_ids = input_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a16a0ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "gen_tokens = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4298458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there a God? (and what about science?) A series of articles on whether Jesus Christ is a “god” and what science can tell us about Jesus. I believe there is a God. I’m a Christian, and I believe in Jesus. I believe science can only inform us if Jesus Christ is not.\n",
      "\n",
      "I have been accused of being a Christian apologist. I respond:\n",
      "\n",
      "Apologists for the supernatural are not Christians. They believe in theism\n"
     ]
    }
   ],
   "source": [
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc7f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_vid",
   "language": "python",
   "name": "gpt_vid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
